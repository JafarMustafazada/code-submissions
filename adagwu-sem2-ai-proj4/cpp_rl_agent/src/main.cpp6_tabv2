#include "jdevtools/jdevcurl.hpp"
#include "nlohmann/json.hpp"

#include <algorithm>
#include <cmath>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <random>
#include <string>
#include <unordered_map>
#include <vector>
#include <thread>

using json = nlohmann::json;
using namespace jdevtools;
using namespace std;


static constexpr int TIME_DELAY = 1;
static constexpr int GRID_SIZE = 40;
static constexpr int N = 0, E = 1, S = 2, W = 3; // Direction indices
const vector<char> DIRECTIONS = {'N', 'E', 'S', 'W'};
const vector<char> DIRECTIONS2 = {'^', '>', 'v', '<'};

// helpers
int json_get_int(json &js, string key) {
	if (js[key].is_number()) return js[key].get<int>();
	return stoi(js[key].get<string>());
}

// API
class GridAPI {
public:
	inline static int userid1 = 3671;
	inline static int teamid1 = 1447;
	inline static int worldid1 = 1;
	inline static vector<string> haeders;

	static void readyH() {
		string apikey = "";
		{
			ifstream file("apikey.txt");
			if (!file) {
				cout << "\nno apikey.\n";
				return;
			}
			getline(file, apikey);
		}
		haeders = {
			"Content-Type: application/x-www-form-urlencoded",
			"x-api-key: " + apikey,
			"userId: " + to_string(userid1)};
	}

	static pair<pair<int, int>, double> makeMove(char direction) {
		requestData req;
		req.headers = haeders;
		req.url = "https://www.notexponential.com/aip2pgaming/api/rl/gw.php";
		req.postData = "type=move&teamId=" + to_string(teamid1) + "&worldId=" + to_string(worldid1) + "&move=" + direction;

		string str = sender(req, (req.postData.size()));
		cout << str;
		json js = json::parse(str);

		if (!js.contains("reward")) {
			cout << "\nno reward\n";
			return {{-1, -1}, 0.0};
		}

		double reward = js["reward"];
		int r = -1, c = -1;
		r = json_get_int(js["newState"], "x");
		c = json_get_int(js["newState"], "y");
		return {{r, c}, reward};
	}

	static pair<int, int> getInitialPosition() {
		requestData req;
		req.headers = haeders;
		req.url = "https://www.notexponential.com/aip2pgaming/api/rl/gw.php?type=location&teamId=" + to_string(teamid1);

		string str = sender(req, (req.postData.size()));
		cout << str << '\n';
		json js = json::parse(str);

		int world = -1, r = -1, c = -1;
		world = json_get_int(js, "world");

		if (world != -1) {
			requestData en;
			en.headers = haeders;
			en.url = "https://www.notexponential.com/aip2pgaming/api/rl/gw.php";
			en.postData = "type=enter&worldId=" + to_string(worldid1) + "&teamId=" + to_string(teamid1);
			string str = sender(req, (req.postData.size()));
			cout << str << '\n';
		}

		if (world != worldid1) {
			cout << "\n error. current is " << world << " while iteration is " << worldid1 << '\n';
			return {-1, -1};
		}

		str = js["state"].get<string>();
		size_t p1 = str.find(':');
		r = stoi(str.substr(0, p1));
		c = stoi(str.substr(p1 + 1));

		return {r, c};
	}
};

class QLearningSolver {
private:
	// Q-table: maps state-action pairs to expected rewards
	unordered_map<string, vector<double> > qTable;

	// Current position
	pair<int, int> currentPos;

	// Learning parameters
	double alpha = 0.1;          // Learning rate
	double gamma = 0.99;         // Discount factor
	double epsilon = 0.1;        // Exploration rate
	double epsilonDecay = 0.999; // Decay rate for epsilon
	double minEpsilon = 0.01;    // Minimum exploration rate

	// Statistics
	int episodeCount = 0;
	bool targetFounded = false;

	// Random number generator
	random_device rd;
	mt19937 rng;

	// Convert position to state string
	string stateToString(const pair<int, int> &pos) {
		return to_string(pos.first) + ":" + to_string(pos.second);
	}

	// Choose action using epsilon-greedy policy
	int chooseAction(const string &state) {
		uniform_real_distribution<double> dist(0.0, 1.0);

		// Ensure state exists in Q-table
		if (qTable.find(state) == qTable.end()) {
			qTable[state] = vector<double>(4, 0.0); // Initialize with zeros
		}

		// Epsilon-greedy action selection
		if (dist(rng) < epsilon) {
			// Exploration: choose random action
			uniform_int_distribution<int> actionDist(0, 3);
			return actionDist(rng);
		} else {
			// Exploitation: choose best action
			auto &qValues = qTable[state];
			return max_element(qValues.begin(), qValues.end()) - qValues.begin();
		}
	}

	// Update Q-value for a state-action pair
	void updateQValue(const string &state, int action, double reward, const string &nextState) {
		// Ensure states exist in Q-table
		if (qTable.find(state) == qTable.end()) {
			qTable[state] = vector<double>(4, 0.0f);
		}
		if (qTable.find(nextState) == qTable.end()) {
			qTable[nextState] = vector<double>(4, 0.0f);
		}

		// Get maximum Q-value for next state
		double maxNextQ = *max_element(qTable[nextState].begin(), qTable[nextState].end());

		// Q-learning update rule
		qTable[state][action] = (1 - alpha) * qTable[state][action] +
								alpha * (reward + gamma * maxNextQ);
	}

	// Decay epsilon for exploration rate
	void decayEpsilon() {
		epsilon = max(minEpsilon, epsilon * epsilonDecay);
	}
	
public:
	// Load Q-table from file
	void loadQTable() {
		ifstream file(("world_" + to_string(GridAPI::worldid1) + "_tabv2.json"));
		if (!file.is_open()) {
			cout << "No existing Q-table found, starting fresh." << endl;
			return;
		}
		json js;
		file >> js;
		qTable = js["Q"].get<unordered_map<string, vector<double> > >();
		epsilon = js["eps"].get<double>();
		episodeCount = js["trained"];
		targetFounded = js["founded"];
		file.close();
		cout << "Loaded Q-table with " << qTable.size() << " states." << endl;
	}

	// Save Q-table to file
	void saveQTable() {
		ofstream file(("world_" + to_string(GridAPI::worldid1) + "_tabv2.json"));
		if (!file.is_open()) {
			cerr << "Cannot open file for saving Q-table." << endl;
			return;
		}
		json js;
		js["Q"] = qTable;
		js["eps"] = epsilon;
		js["trained"] = episodeCount;
		js["founded"] = targetFounded;
		file << js.dump(2);
		file.close();
	}

	QLearningSolver() : rng(rd()) {
		// loadQTable(); // Attempt to load existing Q-table
	}

	~QLearningSolver() {
		// Save Q-table when done
		saveQTable();
	}

	// Run a training episode
	void runEpisode(int maxSteps = 1000) {
		int steps = 0;

		// Reset to initial position for this episode
		currentPos = GridAPI::getInitialPosition();
		string state = stateToString(currentPos);

		while (steps < maxSteps) {
			auto wait_until = std::chrono::system_clock::now() + std::chrono::seconds(TIME_DELAY);
			steps++;

			// Choose action using epsilon-greedy
			int action = chooseAction(state);
			char direction = DIRECTIONS[action];

			// Take action and observe result
			auto [newPos, reward] = GridAPI::makeMove(direction);
			cout << " " << DIRECTIONS2[action];
			string nextState = stateToString(newPos);

			// Check if target found
			if (reward >= 1000) {
				targetFounded = true;
				cout << "\nTarget found in episode " << episodeCount
						<< " after " << steps << " steps!" << endl;
			}

			// Update Q-value
			updateQValue(state, action, reward, nextState);
			saveQTable();

			if (targetFounded) break;
			if (steps % 100 == 0) {
				printStats();
				cout << "Steps taken: " << steps << endl;
			}
			// Update current state and position
			state = nextState;
			currentPos = newPos;
			
			cout << "\nasleep..";
			std::this_thread::sleep_until(wait_until);
			cout << "awake.. ";
		}

		// Decay exploration rate
		decayEpsilon();
		episodeCount++;
		saveQTable();

		if (!targetFounded) {
			cout << "Episode " << episodeCount << " completed with "
					<< steps << " steps. No target found." << endl;
		}
	}

	// Find the optimal path to the target using learned Q-values
	void findOptimalPath(int maxSteps = 1000) {
		if (!targetFounded) cout << "\nTarget yet to be founded" << endl;
		cout << "Finding optimal path to target..." << endl;

		// Reset to initial position
		currentPos = GridAPI::getInitialPosition();
		string state = stateToString(currentPos);

		int steps = 0;
		bool foundTarget = false;

		// Follow the policy greedily (no exploration)
		while (steps < maxSteps) {
			auto wait_until = std::chrono::system_clock::now() + std::chrono::seconds(TIME_DELAY);
			steps++;

			// Ensure state exists in Q-table
			if (qTable.find(state) == qTable.end()) {
				cout << "Warning: State " << state << " not in Q-table." << endl;
				qTable[state] = vector<double>(4, 0.0f);
			}

			// Choose the best action according to Q-values
			auto &qValues = qTable[state];
			int bestAction = max_element(qValues.begin(), qValues.end()) - qValues.begin();
			char direction = DIRECTIONS[bestAction];

			cout << "Step " << steps << ": Moving " << direction << " from "
					<< currentPos.first << "," << currentPos.second << endl;

			// Take action
			auto [newPos, reward] = GridAPI::makeMove(direction);

			// Check if target found
			if (reward >= 1000) {
				foundTarget = true;
				cout << "Target reached after " << steps << " steps!" << endl;
				break;
			}

			// Update state and position
			state = stateToString(newPos);
			currentPos = newPos;

			cout << "asleep..";
			std::this_thread::sleep_until(wait_until);
			cout << "awake.. ";
		}

		if (!foundTarget) {
			cout << "Failed to find target within " << maxSteps << " steps." << endl;
		}
	}

	// Train the Q-learner
	void train(int episodes = 100) {
		cout << "Starting Q-learning training for " << episodes << " episodes..." << endl;

		for (int i = episodeCount; i < episodes; ++i) {
			runEpisode();
			printStats();
		}

		cout << "Training complete. Final statistics:" << endl;
		printStats();
	}

	// Print statistics
	void printStats() {
		cout << "\n===== Q-Learning Statistics =====" << endl;
		cout << "Episodes completed: " << episodeCount << endl;
		cout << "Current exploration rate: " << epsilon << endl;
		cout << "=================================" << endl;
	}

	// Visualize the learned policy for a region of the grid
	void visualizePolicy(int centerX = -1, int centerY = -1, int radius = 5) {
		if (centerX < 0) centerX = currentPos.first;
		if (centerY < 0) centerY = currentPos.second;

		int minX = max(0, centerX - radius);
		int maxX = min(GRID_SIZE - 1, centerX + radius);
		int minY = max(0, centerY - radius);
		int maxY = min(GRID_SIZE - 1, centerY + radius);

		cout << "Policy visualization (region [" << minX << "," << minY
				<< "] to [" << maxX << "," << maxY << "]):" << endl;

		// Print column headers
		cout << "   ";
		for (int j = minY; j <= maxY; ++j) {
			cout << (j % 10) << "  ";
		}
		cout << endl;

		// Print grid with best actions
		for (int i = minX; i <= maxX; ++i) {
			// Print row header
			cout << (i % 10) << ": ";

			for (int j = minY; j <= maxY; ++j) {
				string state = to_string(i) + "," + to_string(j);

				if (i == currentPos.first && j == currentPos.second) {
					cout << "C  "; // Current position
				} else if (qTable.find(state) != qTable.end()) {
					auto &qValues = qTable[state];
					int bestAction = max_element(qValues.begin(), qValues.end()) - qValues.begin();
					char actionChar;
					switch (bestAction) {
					case N:
						actionChar = '^';
						break;
					case E:
						actionChar = '>';
						break;
					case S:
						actionChar = 'v';
						break;
					case W:
						actionChar = '<';
						break;
					default:
						actionChar = '?';
					}
					cout << actionChar << "  ";
				} else {
					cout << ".  "; // Unknown
				}
			}
			cout << endl;
		}
	}

	// Adjust learning parameters
	void setParameters(double newAlpha, double newGamma, double newEpsilon,
					double newEpsilonDecay, double newMinEpsilon) {
		alpha = newAlpha;
		gamma = newGamma;
		epsilon = newEpsilon;
		epsilonDecay = newEpsilonDecay;
		minEpsilon = newMinEpsilon;

		cout << "Learning parameters updated:" << endl;
		cout << "Alpha (learning rate): " << alpha << endl;
		cout << "Gamma (discount factor): " << gamma << endl;
		cout << "Epsilon (exploration rate): " << epsilon << endl;
		cout << "Epsilon decay rate: " << epsilonDecay << endl;
		cout << "Minimum epsilon: " << minEpsilon << endl;
	}
};

int main(int argc, char** argv) {
	int world1 = 1, userid1 = 3671, teamid1 = 1447;
	// int always1 = 0;
	// int alpha0, eps0, tau0;
	// bool feature = false, boltzman = false;

	std::string argument = (argc > 1) ? (argv[1]) : ("-help");

	std::cout << "total arguments: " << int((argc - 1) / 2) << "\n";
	if (argument == "-help") {
		std::cout << "\nneed at least one argument to start like: -userid\n";
		std::cout << "-userid {default(3671)}\n";
		std::cout << "-teamid {default(1447)}\n";
		std::cout << "-world {which world we learning. default(1)}\n";
		return 0;
	}

	argc--;
	for (int i = 1; i < argc; i += 2) {
		std::string argument = argv[i];
		std::cout << argument << " " << argv[i + 1] << "\n";
		if (false) ;
		else if (argument == "-userid") userid1 = std::stoi(argv[i + 1]);
		else if (argument == "-teamid") teamid1 = std::stoi(argv[i + 1]);
		else if (argument == "-world") world1 = std::stoi(argv[i + 1]);
		else {
			std::cout << "Error with param:{" << argument << "}\n";
			return -1;
		}
	}

	GridAPI::teamid1 = teamid1;
	GridAPI::userid1 = userid1;
	GridAPI::worldid1 = world1;
	GridAPI::readyH();

	cout << "Starting Q-Learning Grid Explorer..." << endl;
	QLearningSolver solver;

	solver.setParameters(0.2, 0.95, 0.5, 0.995, 0.01);
	solver.loadQTable();
	solver.train(200); // Train for 200 episodes

	// Visualize the learned policy
	solver.visualizePolicy();

	// Find optimal path to the target
	solver.findOptimalPath();

	cout << "Program complete." << endl;
	return 0;
}